{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Tugas 3"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Regresi"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport statsmodels\nimport statsmodels.api as sm",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset=pd.read_csv('qfta_missingvalue.csv')\nx=dataset.iloc[:,3:13].values\ny=dataset.iloc[:,13].values",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_train, x_test, y_train, y_test=train_test_split(x,\n                                                 y,\n                                                 test_size=1/3,\n                                                 random_state=0)",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "regressor=LinearRegression()\nregressor.fit(x_train, y_train)",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred=regressor.predict(x_test)\nprint(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "332445.27665375994\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X=sm.add_constant(x)\nmodel=sm.OLS(y,X).fit()\npredictions=model.predict(X)\nmodel.summary()",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.733</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.661</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   10.16</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 10 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>6.16e-08</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:04:21</td>     <th>  Log-Likelihood:    </th> <td> -663.36</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    48</td>      <th>  AIC:               </th> <td>   1349.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    37</td>      <th>  BIC:               </th> <td>   1369.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 6.209e+05</td> <td> 6.94e+05</td> <td>    0.894</td> <td> 0.377</td> <td>-7.86e+05</td> <td> 2.03e+06</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>  137.2474</td> <td>  110.882</td> <td>    1.238</td> <td> 0.224</td> <td>  -87.422</td> <td>  361.917</td>\n</tr>\n<tr>\n  <th>x2</th>    <td> -333.7656</td> <td>  364.058</td> <td>   -0.917</td> <td> 0.365</td> <td>-1071.418</td> <td>  403.887</td>\n</tr>\n<tr>\n  <th>x3</th>    <td> -105.8141</td> <td>  145.921</td> <td>   -0.725</td> <td> 0.473</td> <td> -401.479</td> <td>  189.851</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>  206.4581</td> <td>  533.269</td> <td>    0.387</td> <td> 0.701</td> <td> -874.048</td> <td> 1286.964</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>  733.0939</td> <td>  699.646</td> <td>    1.048</td> <td> 0.302</td> <td> -684.525</td> <td> 2150.712</td>\n</tr>\n<tr>\n  <th>x6</th>    <td>   44.8635</td> <td>   62.866</td> <td>    0.714</td> <td> 0.480</td> <td>  -82.516</td> <td>  172.243</td>\n</tr>\n<tr>\n  <th>x7</th>    <td> -262.9602</td> <td>  138.842</td> <td>   -1.894</td> <td> 0.066</td> <td> -544.281</td> <td>   18.360</td>\n</tr>\n<tr>\n  <th>x8</th>    <td> -292.9801</td> <td>   82.819</td> <td>   -3.538</td> <td> 0.001</td> <td> -460.787</td> <td> -125.173</td>\n</tr>\n<tr>\n  <th>x9</th>    <td> -417.7147</td> <td>  492.684</td> <td>   -0.848</td> <td> 0.402</td> <td>-1415.987</td> <td>  580.558</td>\n</tr>\n<tr>\n  <th>x10</th>   <td>   13.8782</td> <td>    7.516</td> <td>    1.847</td> <td> 0.073</td> <td>   -1.350</td> <td>   29.106</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 3.024</td> <th>  Durbin-Watson:     </th> <td>   2.070</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.220</td> <th>  Jarque-Bera (JB):  </th> <td>   2.472</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.431</td> <th>  Prob(JB):          </th> <td>   0.291</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.297</td> <th>  Cond. No.          </th> <td>6.07e+05</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.07e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems.",
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.733\nModel:                            OLS   Adj. R-squared:                  0.661\nMethod:                 Least Squares   F-statistic:                     10.16\nDate:                Sun, 10 Mar 2019   Prob (F-statistic):           6.16e-08\nTime:                        15:04:21   Log-Likelihood:                -663.36\nNo. Observations:                  48   AIC:                             1349.\nDf Residuals:                      37   BIC:                             1369.\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       6.209e+05   6.94e+05      0.894      0.377   -7.86e+05    2.03e+06\nx1           137.2474    110.882      1.238      0.224     -87.422     361.917\nx2          -333.7656    364.058     -0.917      0.365   -1071.418     403.887\nx3          -105.8141    145.921     -0.725      0.473    -401.479     189.851\nx4           206.4581    533.269      0.387      0.701    -874.048    1286.964\nx5           733.0939    699.646      1.048      0.302    -684.525    2150.712\nx6            44.8635     62.866      0.714      0.480     -82.516     172.243\nx7          -262.9602    138.842     -1.894      0.066    -544.281      18.360\nx8          -292.9801     82.819     -3.538      0.001    -460.787    -125.173\nx9          -417.7147    492.684     -0.848      0.402   -1415.987     580.558\nx10           13.8782      7.516      1.847      0.073      -1.350      29.106\n==============================================================================\nOmnibus:                        3.024   Durbin-Watson:                   2.070\nProb(Omnibus):                  0.220   Jarque-Bera (JB):                2.472\nSkew:                          -0.431   Prob(JB):                        0.291\nKurtosis:                       2.297   Cond. No.                     6.07e+05\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.07e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset1=pd.read_csv('qfta_missvalout.csv')\nx1=dataset1.iloc[:,3:13].values\ny1=dataset1.iloc[:,13].values",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_train, x_test, y_train, y_test=train_test_split(x1,\n                                                 y1,\n                                                 test_size=1/3,\n                                                 random_state=0)",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "regressor=LinearRegression()\nregressor.fit(x_train, y_train)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred=regressor.predict(x_test)\nprint(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": "143459.1927324259\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X1=sm.add_constant(x1)\nmodel1=sm.OLS(y1,X1).fit()\npredictions1=model.predict(X1)\nmodel1.summary()",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.913</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.889</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   37.72</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 10 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>4.33e-16</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:04:28</td>     <th>  Log-Likelihood:    </th> <td> -623.68</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    47</td>      <th>  AIC:               </th> <td>   1269.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    36</td>      <th>  BIC:               </th> <td>   1290.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 1.279e+06</td> <td> 4.09e+05</td> <td>    3.128</td> <td> 0.003</td> <td>  4.5e+05</td> <td> 2.11e+06</td>\n</tr>\n<tr>\n  <th>x1</th>    <td> -202.2296</td> <td>   75.269</td> <td>   -2.687</td> <td> 0.011</td> <td> -354.882</td> <td>  -49.577</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>   44.2832</td> <td>  215.197</td> <td>    0.206</td> <td> 0.838</td> <td> -392.157</td> <td>  480.724</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>   68.3333</td> <td>   86.827</td> <td>    0.787</td> <td> 0.436</td> <td> -107.759</td> <td>  244.426</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>  694.9607</td> <td>  313.767</td> <td>    2.215</td> <td> 0.033</td> <td>   58.612</td> <td> 1331.309</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>  618.4601</td> <td>  405.124</td> <td>    1.527</td> <td> 0.136</td> <td> -203.169</td> <td> 1440.089</td>\n</tr>\n<tr>\n  <th>x6</th>    <td>    2.4818</td> <td>   36.713</td> <td>    0.068</td> <td> 0.946</td> <td>  -71.975</td> <td>   76.938</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>  -76.4581</td> <td>   83.207</td> <td>   -0.919</td> <td> 0.364</td> <td> -245.211</td> <td>   92.295</td>\n</tr>\n<tr>\n  <th>x8</th>    <td> -107.9203</td> <td>   52.508</td> <td>   -2.055</td> <td> 0.047</td> <td> -214.412</td> <td>   -1.429</td>\n</tr>\n<tr>\n  <th>x9</th>    <td> -143.8713</td> <td>  286.891</td> <td>   -0.501</td> <td> 0.619</td> <td> -725.713</td> <td>  437.970</td>\n</tr>\n<tr>\n  <th>x10</th>   <td>   78.1301</td> <td>    8.623</td> <td>    9.061</td> <td> 0.000</td> <td>   60.642</td> <td>   95.618</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 5.553</td> <th>  Durbin-Watson:     </th> <td>   1.609</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.062</td> <th>  Jarque-Bera (JB):  </th> <td>   2.900</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.368</td> <th>  Prob(JB):          </th> <td>   0.235</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.030</td> <th>  Cond. No.          </th> <td>5.87e+05</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.87e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems.",
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.913\nModel:                            OLS   Adj. R-squared:                  0.889\nMethod:                 Least Squares   F-statistic:                     37.72\nDate:                Sun, 10 Mar 2019   Prob (F-statistic):           4.33e-16\nTime:                        15:04:28   Log-Likelihood:                -623.68\nNo. Observations:                  47   AIC:                             1269.\nDf Residuals:                      36   BIC:                             1290.\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       1.279e+06   4.09e+05      3.128      0.003     4.5e+05    2.11e+06\nx1          -202.2296     75.269     -2.687      0.011    -354.882     -49.577\nx2            44.2832    215.197      0.206      0.838    -392.157     480.724\nx3            68.3333     86.827      0.787      0.436    -107.759     244.426\nx4           694.9607    313.767      2.215      0.033      58.612    1331.309\nx5           618.4601    405.124      1.527      0.136    -203.169    1440.089\nx6             2.4818     36.713      0.068      0.946     -71.975      76.938\nx7           -76.4581     83.207     -0.919      0.364    -245.211      92.295\nx8          -107.9203     52.508     -2.055      0.047    -214.412      -1.429\nx9          -143.8713    286.891     -0.501      0.619    -725.713     437.970\nx10           78.1301      8.623      9.061      0.000      60.642      95.618\n==============================================================================\nOmnibus:                        5.553   Durbin-Watson:                   1.609\nProb(Omnibus):                  0.062   Jarque-Bera (JB):                2.900\nSkew:                           0.368   Prob(JB):                        0.235\nKurtosis:                       2.030   Cond. No.                     5.87e+05\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.87e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset2=pd.read_csv('qfta_xnorm.csv')\nx2=dataset2.iloc[:,1:11].values\ny2=dataset2.iloc[:,11].values",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test=train_test_split(x2,\n                                                 y2,\n                                                 test_size=1/3,\n                                                 random_state=0)",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(x_train, y_train)",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred=regressor.predict(x_test)\nfrom sklearn import metrics\nprint(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2.787188379283682e-05\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X2=sm.add_constant(x2)\nmodel2=sm.OLS(y2,X2).fit()\npredictions=model.predict(X2)\nmodel2.summary()",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.973</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.966</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   134.7</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 10 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>5.77e-26</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:04:36</td>     <th>  Log-Likelihood:    </th> <td>  439.07</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    48</td>      <th>  AIC:               </th> <td>  -856.1</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    37</td>      <th>  BIC:               </th> <td>  -835.6</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>    1.0004</td> <td> 2.58e-05</td> <td> 3.88e+04</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.0441</td> <td>    0.016</td> <td>    2.717</td> <td> 0.010</td> <td>    0.011</td> <td>    0.077</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>   -0.1808</td> <td>    0.026</td> <td>   -7.011</td> <td> 0.000</td> <td>   -0.233</td> <td>   -0.129</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>   -0.0381</td> <td>    0.027</td> <td>   -1.434</td> <td> 0.160</td> <td>   -0.092</td> <td>    0.016</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>   -0.1750</td> <td>    0.087</td> <td>   -2.002</td> <td> 0.053</td> <td>   -0.352</td> <td>    0.002</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>    0.0634</td> <td>    0.100</td> <td>    0.634</td> <td> 0.530</td> <td>   -0.139</td> <td>    0.266</td>\n</tr>\n<tr>\n  <th>x6</th>    <td>   -0.0100</td> <td>    0.009</td> <td>   -1.142</td> <td> 0.261</td> <td>   -0.028</td> <td>    0.008</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>   -0.0056</td> <td>    0.029</td> <td>   -0.194</td> <td> 0.847</td> <td>   -0.065</td> <td>    0.053</td>\n</tr>\n<tr>\n  <th>x8</th>    <td>    0.0042</td> <td>    0.015</td> <td>    0.283</td> <td> 0.779</td> <td>   -0.026</td> <td>    0.035</td>\n</tr>\n<tr>\n  <th>x9</th>    <td>    0.0642</td> <td>    0.073</td> <td>    0.884</td> <td> 0.383</td> <td>   -0.083</td> <td>    0.211</td>\n</tr>\n<tr>\n  <th>x10</th>   <td>   -0.0292</td> <td>    0.001</td> <td>  -22.071</td> <td> 0.000</td> <td>   -0.032</td> <td>   -0.026</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>36.351</td> <th>  Durbin-Watson:     </th> <td>   1.844</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 108.769</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-2.004</td> <th>  Prob(JB):          </th> <td>2.41e-24</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 9.190</td> <th>  Cond. No.          </th> <td>2.44e+04</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.44e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.",
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.973\nModel:                            OLS   Adj. R-squared:                  0.966\nMethod:                 Least Squares   F-statistic:                     134.7\nDate:                Sun, 10 Mar 2019   Prob (F-statistic):           5.77e-26\nTime:                        15:04:36   Log-Likelihood:                 439.07\nNo. Observations:                  48   AIC:                            -856.1\nDf Residuals:                      37   BIC:                            -835.6\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0004   2.58e-05   3.88e+04      0.000       1.000       1.000\nx1             0.0441      0.016      2.717      0.010       0.011       0.077\nx2            -0.1808      0.026     -7.011      0.000      -0.233      -0.129\nx3            -0.0381      0.027     -1.434      0.160      -0.092       0.016\nx4            -0.1750      0.087     -2.002      0.053      -0.352       0.002\nx5             0.0634      0.100      0.634      0.530      -0.139       0.266\nx6            -0.0100      0.009     -1.142      0.261      -0.028       0.008\nx7            -0.0056      0.029     -0.194      0.847      -0.065       0.053\nx8             0.0042      0.015      0.283      0.779      -0.026       0.035\nx9             0.0642      0.073      0.884      0.383      -0.083       0.211\nx10           -0.0292      0.001    -22.071      0.000      -0.032      -0.026\n==============================================================================\nOmnibus:                       36.351   Durbin-Watson:                   1.844\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              108.769\nSkew:                          -2.004   Prob(JB):                     2.41e-24\nKurtosis:                       9.190   Cond. No.                     2.44e+04\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.44e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset3=pd.read_csv('qfta_ynorm.csv')\nx3=dataset3.iloc[:,1:11].values\ny3=dataset3.iloc[:,11].values",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test=train_test_split(x3,\n                                                 y3,\n                                                 test_size=1/3,\n                                                 random_state=0)",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(x_train, y_train)",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred=regressor.predict(x_test)\nfrom sklearn import metrics\nprint(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "7.927160940039948e-06\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X3=sm.add_constant(x3)\nmodel3=sm.OLS(y3,X3).fit()\npredictions=model.predict(X3)\nmodel3.summary()",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.995</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.993</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   696.5</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 10 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>4.53e-38</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:04:44</td>     <th>  Log-Likelihood:    </th> <td>  500.30</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    47</td>      <th>  AIC:               </th> <td>  -978.6</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    36</td>      <th>  BIC:               </th> <td>  -958.2</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>    1.0002</td> <td> 8.69e-06</td> <td> 1.15e+05</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>   -0.0289</td> <td>    0.005</td> <td>   -6.316</td> <td> 0.000</td> <td>   -0.038</td> <td>   -0.020</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.0043</td> <td>    0.009</td> <td>    0.476</td> <td> 0.637</td> <td>   -0.014</td> <td>    0.023</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>    0.0040</td> <td>    0.006</td> <td>    0.643</td> <td> 0.524</td> <td>   -0.009</td> <td>    0.016</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>    0.0150</td> <td>    0.021</td> <td>    0.718</td> <td> 0.477</td> <td>   -0.027</td> <td>    0.057</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>    0.0699</td> <td>    0.022</td> <td>    3.110</td> <td> 0.004</td> <td>    0.024</td> <td>    0.115</td>\n</tr>\n<tr>\n  <th>x6</th>    <td>   -0.0091</td> <td>    0.002</td> <td>   -4.608</td> <td> 0.000</td> <td>   -0.013</td> <td>   -0.005</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>   -0.0022</td> <td>    0.007</td> <td>   -0.333</td> <td> 0.741</td> <td>   -0.015</td> <td>    0.011</td>\n</tr>\n<tr>\n  <th>x8</th>    <td>   -0.0054</td> <td>    0.003</td> <td>   -1.592</td> <td> 0.120</td> <td>   -0.012</td> <td>    0.001</td>\n</tr>\n<tr>\n  <th>x9</th>    <td>    0.0298</td> <td>    0.016</td> <td>    1.823</td> <td> 0.077</td> <td>   -0.003</td> <td>    0.063</td>\n</tr>\n<tr>\n  <th>x10</th>   <td>   -0.0100</td> <td>    0.001</td> <td>  -12.802</td> <td> 0.000</td> <td>   -0.012</td> <td>   -0.008</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.569</td> <th>  Durbin-Watson:     </th> <td>   1.545</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.752</td> <th>  Jarque-Bera (JB):  </th> <td>   0.415</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.225</td> <th>  Prob(JB):          </th> <td>   0.813</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.907</td> <th>  Cond. No.          </th> <td>2.42e+04</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.42e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.",
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.995\nModel:                            OLS   Adj. R-squared:                  0.993\nMethod:                 Least Squares   F-statistic:                     696.5\nDate:                Sun, 10 Mar 2019   Prob (F-statistic):           4.53e-38\nTime:                        15:04:44   Log-Likelihood:                 500.30\nNo. Observations:                  47   AIC:                            -978.6\nDf Residuals:                      36   BIC:                            -958.2\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0002   8.69e-06   1.15e+05      0.000       1.000       1.000\nx1            -0.0289      0.005     -6.316      0.000      -0.038      -0.020\nx2             0.0043      0.009      0.476      0.637      -0.014       0.023\nx3             0.0040      0.006      0.643      0.524      -0.009       0.016\nx4             0.0150      0.021      0.718      0.477      -0.027       0.057\nx5             0.0699      0.022      3.110      0.004       0.024       0.115\nx6            -0.0091      0.002     -4.608      0.000      -0.013      -0.005\nx7            -0.0022      0.007     -0.333      0.741      -0.015       0.011\nx8            -0.0054      0.003     -1.592      0.120      -0.012       0.001\nx9             0.0298      0.016      1.823      0.077      -0.003       0.063\nx10           -0.0100      0.001    -12.802      0.000      -0.012      -0.008\n==============================================================================\nOmnibus:                        0.569   Durbin-Watson:                   1.545\nProb(Omnibus):                  0.752   Jarque-Bera (JB):                0.415\nSkew:                          -0.225   Prob(JB):                        0.813\nKurtosis:                       2.907   Cond. No.                     2.42e+04\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.42e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}